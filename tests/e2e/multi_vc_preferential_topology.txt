// /*
// 	Copyright 2020 The Kubernetes Authors.

// 	Licensed under the Apache License, Version 2.0 (the "License");
// 	you may not use this file except in compliance with the License.
// 	You may obtain a copy of the License at

// 		http://www.apache.org/licenses/LICENSE-2.0

// 	Unless required by applicable law or agreed to in writing, software
// 	distributed under the License is distributed on an "AS IS" BASIS,
// 	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// 	See the License for the specific language governing permissions and
// 	limitations under the License.
// */

// package e2e

// import (
// 	"context"
// 	"fmt"
// 	"time"

// 	"github.com/onsi/ginkgo/v2"
// 	"github.com/onsi/gomega"
// 	"github.com/vmware/govmomi/object"
// 	"golang.org/x/crypto/ssh"
// 	v1 "k8s.io/api/core/v1"
// 	apierrors "k8s.io/apimachinery/pkg/api/errors"
// 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
// 	clientset "k8s.io/client-go/kubernetes"
// 	"k8s.io/kubernetes/test/e2e/framework"
// 	fnodes "k8s.io/kubernetes/test/e2e/framework/node"
// 	fpod "k8s.io/kubernetes/test/e2e/framework/pod"
// 	fpv "k8s.io/kubernetes/test/e2e/framework/pv"
// 	fss "k8s.io/kubernetes/test/e2e/framework/statefulset"
// 	admissionapi "k8s.io/pod-security-admission/api"
// )

// var _ = ginkgo.Describe("[Preferential-Topology] Preferential-Topology-Provisioning", func() {
// 	f := framework.NewDefaultFramework("preferential-topology-aware-provisioning")
// 	f.NamespacePodSecurityEnforceLevel = admissionapi.LevelPrivileged
// 	var (
// 		client    clientset.Interface
// 		namespace string

// 		topologyLength int

// 		allowedTopologies        []v1.TopologySelectorLabelRequirement
// 		preferredDatastoreChosen int

// 		nonShareddatastoreListMapRack1 map[string]string
// 		nonShareddatastoreListMapRack2 map[string]string
// 		allMasterIps                   []string
// 		masterIp                       string

// 		dataCenters             []*object.Datacenter
// 		clusters                []string
// 		csiReplicas             int32
// 		csiNamespace            string
// 		preferredDatastorePaths []string
// 		allowedTopologyRacks    []string
// 		allowedTopologyForRack1 []v1.TopologySelectorLabelRequirement
// 		allowedTopologyForRack2 []v1.TopologySelectorLabelRequirement
// 		sshClientConfig         *ssh.ClientConfig
// 		nimbusGeneratedK8sVmPwd string
// 	)

// 	ginkgo.BeforeEach(func() {
// 		var cancel context.CancelFunc
// 		ctx, cancel := context.WithCancel(context.Background())
// 		defer cancel()
// 		client = f.ClientSet
// 		namespace = f.Namespace.Name
// 		bootstrap()
// 		sc, err := client.StorageV1().StorageClasses().Get(ctx, defaultNginxStorageClassName, metav1.GetOptions{})
// 		if err == nil && sc != nil {
// 			gomega.Expect(client.StorageV1().StorageClasses().Delete(ctx, sc.Name,
// 				*metav1.NewDeleteOptions(0))).NotTo(gomega.HaveOccurred())
// 		}
// 		nodeList, err := fnodes.GetReadySchedulableNodes(f.ClientSet)
// 		framework.ExpectNoError(err, "Unable to find ready and schedulable Node")
// 		if !(len(nodeList.Items) > 0) {
// 			framework.Failf("Unable to find ready and schedulable Node")
// 		}
// 		csiNamespace = GetAndExpectStringEnvVar(envCSINamespace)

// 		nimbusGeneratedK8sVmPwd = GetAndExpectStringEnvVar(nimbusK8sVmPwd)

// 		sshClientConfig = &ssh.ClientConfig{
// 			User: "root",
// 			Auth: []ssh.AuthMethod{
// 				ssh.Password(nimbusGeneratedK8sVmPwd),
// 			},
// 			HostKeyCallback: ssh.InsecureIgnoreHostKey(),
// 		}

// 		topologyMap := GetAndExpectStringEnvVar(topologyMap)
// 		allowedTopologies = createAllowedTopolgies(topologyMap, topologyLength)

// 		csiDeployment, err := client.AppsV1().Deployments(csiNamespace).Get(
// 			ctx, vSphereCSIControllerPodNamePrefix, metav1.GetOptions{})
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		csiReplicas = *csiDeployment.Spec.Replicas

// 		// fetching k8s master ip
// 		allMasterIps = getK8sMasterIPs(ctx, client)
// 		masterIp = allMasterIps[0]

// 		// fetching datacenter details
// 		dataCenters, err = multiVCe2eVSphere.getAllDatacentersForMultiVC(ctx)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 		// fetching cluster details
// 		clusters, err = getTopologyLevel5ClusterGroupNames(masterIp, sshClientConfig, dataCenters)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 		// fetching list of datastores available in different racks
// 		rack1DatastoreListMap, err = getListOfDatastoresByClusterName(masterIp, sshClientConfig, clusters[0])
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 		//set preferred datatsore time interval
// 		setPreferredDatastoreTimeInterval(client, ctx, csiNamespace, namespace, csiReplicas)

// 	})

// 	ginkgo.AfterEach(func() {
// 		ctx, cancel := context.WithCancel(context.Background())
// 		defer cancel()
// 		ginkgo.By(fmt.Sprintf("Deleting all statefulsets in namespace: %v", namespace))
// 		fss.DeleteAllStatefulSets(client, namespace)
// 		ginkgo.By(fmt.Sprintf("Deleting service nginx in namespace: %v", namespace))
// 		err := client.CoreV1().Services(namespace).Delete(ctx, servicename, *metav1.NewDeleteOptions(0))
// 		if !apierrors.IsNotFound(err) {
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		}
// 		framework.Logf("Perform preferred datastore tags cleanup after test completion")
// 		err = deleteTagCreatedForPreferredDatastore(masterIp, sshClientConfig, allowedTopologyRacks)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 		framework.Logf("Recreate preferred datastore tags post cleanup")
// 		err = createTagForPreferredDatastore(masterIp, sshClientConfig, allowedTopologyRacks)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 	})

// 	/* Testcase-1:
// 		Add preferential tag in all the Availability zone's of   VC1's  and VC2  â†’ change the preference during execution

// 			Steps
// 			Preferential FSS "topology-preferential-datastores" should be set and csi-vsphere-config should have the preferential tag.

// 	    1. Create SC  default parameters with out any topology requirement.
// 	    2. In each availability zone for any one datastore add preferential tag
// 	    Create 3 statefullset with 10 replica's
// 	    wait for all the  PVC to bound and pod's to reach running state
// 	    Verify that since the preffered datastore is available , Volume should get created on the datastores which has the preferencce set
// 	    Make sure common validation points are met on PV,PVC and POD
// 	    Change the Preference in any 2 datastores and Re-start the CSI driver
// 	    scale up the statefull set to 15 replica
// 	    The volumes should get provision on the datastores which has the preference
// 	    Clear the data
// 	*/
// 	ginkgo.It("Tag single preferred datastore each in rack-1 and rack-2 and verify it is honored", func() {
// 		ctx, cancel := context.WithCancel(context.Background())
// 		defer cancel()

// 		scSpec := getVSphereStorageClassSpec(defaultNginxStorageClassName, nil, nil, "",
// 			"", false)
// 		sc, err := client.StorageV1().StorageClasses().Create(ctx, scSpec, metav1.CreateOptions{})
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		defer func() {
// 			err = client.StorageV1().StorageClasses().Delete(ctx, sc.Name, *metav1.NewDeleteOptions(0))
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		}()

// 		preferredDatastoreChosen = 1
// 		preferredDatastorePaths = nil

// 		// choose preferred datastore
// 		ginkgo.By("Tag preferred datastore for volume provisioning in rack-2(cluster-2))")
// 		preferredDatastorePaths, err = tagPreferredDatastore(masterIp, sshClientConfig, allowedTopologyRacks[1],
// 			preferredDatastoreChosen, nonShareddatastoreListMapRack2, nil)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		preferredDatastoreRack2 := preferredDatastorePaths[0]
// 		defer func() {
// 			ginkgo.By("Remove preferred datastore tag")
// 			err = detachTagCreatedOnPreferredDatastore(masterIp, sshClientConfig, preferredDatastoreRack2,
// 				allowedTopologyRacks[1])
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		}()

// 		framework.Logf("Waiting for %v for preferred datastore to get refreshed in the environment",
// 			preferredDatastoreTimeOutInterval)
// 		time.Sleep(preferredDatastoreTimeOutInterval)

// 		ginkgo.By("Creating StorageClass for Statefulset")
// 		scSpec := getVSphereStorageClassSpec(defaultNginxStorageClassName, nil, nil,
// 			"", "", false)
// 		sc, err := client.StorageV1().StorageClasses().Create(ctx, scSpec, metav1.CreateOptions{})
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		defer func() {
// 			err = client.StorageV1().StorageClasses().Delete(ctx, sc.Name, *metav1.NewDeleteOptions(0))
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		}()

// 		ginkgo.By("Creating service")
// 		service := CreateService(namespace, client)
// 		defer func() {
// 			deleteService(namespace, client, service)
// 		}()

// 		// Creating Statefulset
// 		ginkgo.By("Creating statefulset with 3 replica")
// 		statefulset := GetStatefulSetFromManifest(namespace)
// 		ginkgo.By("Creating statefulset")
// 		CreateStatefulSet(namespace, statefulset, client)
// 		replicas := *(statefulset.Spec.Replicas)
// 		defer func() {
// 			framework.Logf("Deleting all statefulset in namespace: %v", namespace)
// 			fss.DeleteAllStatefulSets(client, namespace)
// 		}()

// 		// Waiting for pods status to be Ready.
// 		fss.WaitForStatusReadyReplicas(client, statefulset, replicas)
// 		gomega.Expect(fss.CheckMount(client, statefulset, mountPath)).NotTo(gomega.HaveOccurred())
// 		ssPodsBeforeScaleDown := fss.GetPodList(client, statefulset)
// 		gomega.Expect(ssPodsBeforeScaleDown.Items).NotTo(gomega.BeEmpty(),
// 			fmt.Sprintf("Unable to get list of Pods from the Statefulset: %v", statefulset.Name))
// 		gomega.Expect(len(ssPodsBeforeScaleDown.Items) == int(replicas)).To(gomega.BeTrue(),
// 			"Number of Pods in the statefulset should match with number of replicas")

// 		// verifying volume provisioning
// 		ginkgo.By("Verify volume is provisioned on the preferred datatsore")
// 		err = verifyVolumeProvisioningForStatefulSet(ctx, client, statefulset, namespace, preferredDatastorePaths,
// 			nonShareddatastoreListMapRack2, false, false)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 		ginkgo.By("Verify PV node affinity and that the PODS are running on " +
// 			"appropriate node as specified in the allowed topologies of SC")
// 		verifyPVnodeAffinityAndPODnodedetailsForStatefulsetsLevel5(ctx, client, statefulset,
// 			namespace, allowedTopologyForRack2, false, false)

// 		// choose preferred datastore in rack-1
// 		ginkgo.By("Tag preferred datatstore for volume provisioning in rack-1(cluster-1))")
// 		preferredDatastorePaths, err = tagPreferredDatastore(masterIp, sshClientConfig, allowedTopologyRacks[0],
// 			preferredDatastoreChosen, nonShareddatastoreListMapRack1, nil)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		defer func() {
// 			ginkgo.By("Remove preferred datastore tag")
// 			err = detachTagCreatedOnPreferredDatastore(masterIp, sshClientConfig, preferredDatastorePaths[0],
// 				allowedTopologyRacks[0])
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		}()

// 		framework.Logf("Waiting for %v for preferred datastore to get refreshed in the environment",
// 			preferredDatastoreTimeOutInterval)
// 		time.Sleep(preferredDatastoreTimeOutInterval)

// 		ginkgo.By("Creating Storage class and standalone PVC")
// 		storageclass1, pvclaim, err := createPVCAndStorageClass(client, namespace, nil, nil, "",
// 			allowedTopologyForRack1, "", false, "")
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		defer func() {
// 			err = client.StorageV1().StorageClasses().Delete(ctx, storageclass1.Name, *metav1.NewDeleteOptions(0))
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		}()

// 		//Wait for PVC to reach Bound state
// 		ginkgo.By("Expect claim to provision volume successfully")
// 		pvs1, err := fpv.WaitForPVClaimBoundPhase(client,
// 			[]*v1.PersistentVolumeClaim{pvclaim}, framework.ClaimProvisionTimeout)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred(), "Failed to provision volume")
// 		pv1 := pvs1[0]
// 		defer func() {
// 			err = fpv.DeletePersistentVolumeClaim(client, pvclaim.Name, pvclaim.Namespace)
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 			err = e2eVSphere.waitForCNSVolumeToBeDeleted(pv1.Spec.CSI.VolumeHandle)
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		}()

// 		// Creating Pod and verifying volume is attached to the node
// 		ginkgo.By("Creating a pod")
// 		pod, err := createPod(client, namespace, nil, []*v1.PersistentVolumeClaim{pvclaim}, false, "")
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 		ginkgo.By(fmt.Sprintf("Verify volume:%s is attached to the node: %s",
// 			pv1.Spec.CSI.VolumeHandle, pod.Spec.NodeName))
// 		vmUUID := getNodeUUID(ctx, client, pod.Spec.NodeName)
// 		isDiskAttached, err := e2eVSphere.isVolumeAttachedToVM(client, pv1.Spec.CSI.VolumeHandle, vmUUID)
// 		gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 		gomega.Expect(isDiskAttached).To(gomega.BeTrue(), "Volume is not attached to the node")
// 		defer func() {
// 			ginkgo.By("Deleting the pod and wait for disk to detach")
// 			err := fpod.DeletePodWithWait(client, pod)
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())

// 			ginkgo.By("Verify volume is detached from the node")
// 			isDiskDetached, err := e2eVSphere.waitForVolumeDetachedFromNode(client,
// 				pv1.Spec.CSI.VolumeHandle, pod.Spec.NodeName)
// 			gomega.Expect(err).NotTo(gomega.HaveOccurred())
// 			gomega.Expect(isDiskDetached).To(gomega.BeTrue(),
// 				fmt.Sprintf("Volume %q is not detached from the node %q", pv1.Spec.CSI.VolumeHandle, pod.Spec.NodeName))
// 		}()

// 		// verifying volume provisioning
// 		ginkgo.By("Verify volume is provisioned on the preferred datatsore")
// 		verifyVolumeProvisioningForStandalonePods(ctx, client, pod, namespace, preferredDatastorePaths,
// 			nonShareddatastoreListMapRack1)

// 		ginkgo.By("Verify PV node affinity and that the PODS are running on appropriate " +
// 			"node as specified in the allowed topologies of SC")
// 		verifyPVnodeAffinityAndPODnodedetailsFoStandalonePodLevel5(ctx, client, pod, namespace,
// 			allowedTopologyForRack1, false)
// 	})

// })


func writeConfigToSecretString(cfg e2eTestConfig) (string, error) {
    result := fmt.Sprintf("[Global]\ninsecure-flag = \"%s\"\ncluster-distribution = \"%s\"\nquery-limit = %d\n"+
        "csi-fetch-preferred-datastores-intervalinmin = %d\nlist-volume-threshold = %d\n\n"+
        "[VirtualCenter \"10.161.119.92\"]\ninsecure-flag = \"%s\"\nuser = \"%s\"\npassword = \"%s\"\nport = \"%s\"\n"+
        "datacenters = \"%s\"\n\n"+
        "[VirtualCenter \"10.78.160.225\"]\ninsecure-flag = \"%s\"\nuser = \"%s\"\npassword = \"%s\"\nport = \"%s\"\n"+
        "datacenters = \"%s\"\n\n"+
        "[VirtualCenter \"10.193.3.53\"]\ninsecure-flag = \"%s\"\nuser = \"%s\"\npassword = \"%s\"\nport = \"%s\"\n"+
        "datacenters = \"%s\"\n\n"+
        "[Snapshot]\nglobal-max-snapshots-per-block-volume = %d\n\n"+
        "[Labels]\ntopology-categories = \"%s\"",
        cfg.Global.InsecureFlag, cfg.Global.ClusterDistribution, cfg.Global.QueryLimit,
        cfg.Global.CSIFetchPreferredDatastoresIntervalInMin, cfg.Global.ListVolumeThreshold,
        cfg.VirtualCenter1.InsecureFlag, cfg.VirtualCenter1.User, cfg.VirtualCenter1.Password, cfg.VirtualCenter1.Port,
        cfg.VirtualCenter1.Datacenters,
        cfg.VirtualCenter2.InsecureFlag, cfg.VirtualCenter2.User, cfg.VirtualCenter2.Password, cfg.VirtualCenter2.Port,
        cfg.VirtualCenter2.Datacenters,
        cfg.VirtualCenter3.InsecureFlag, cfg.VirtualCenter3.User, cfg.VirtualCenter3.Password, cfg.VirtualCenter3.Port,
        cfg.VirtualCenter3.Datacenters,
        cfg.Snapshot.GlobalMaxSnapshotsPerBlockVolume,
        cfg.Labels.TopologyCategories)
    return result, nil
}
